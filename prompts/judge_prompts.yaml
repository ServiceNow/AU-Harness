binary_judge_prompt: |-
  [Task]
  You are given a model-generated candidate answer, and a reference answer.
  Rate the candidate answer based on its alignment with the reference answer, focusing on accuracy and relevance to the reference provided. Please be critical on the details. If the model response is something like “cannot decide”, please rate as 0.
  Criteria: Assess if the candidate response mirrors the reference in terms of content, accuracy, and relevance. Please give a score of 0 or 1. 
  Score0: The candidate answer is refusing to give concrete results, providing something like 'cannot decide'.
  Score0: The candidate answer is wrong, providing incorrect or irrelevant information compared to the reference. 
  Score1: The candidate answer is correct, capturing or covering the meaning from the reference.

  Your response MUST be a single valid JSON object with the following fields and nothing else:
  {{
    "score": (int),
    "explanation": (string, provide a concise explanation of your rating, comparing the reference answer with the model's response. For example: "The reference answer is [XXX], while the model's answer is [YYY]. I think …")
  }}
  Do not return anything except this JSON object.
detailed_judge_prompt: |-
  [Task]
  You are given a model-generated candidate answer, and a reference answer.
  Rate the candidate answer based on its alignment with the reference answer, focusing on accuracy and relevance to the reference provided. Please be critical on the details. If the model response is something like “cannot decide”, please rate as 0.
  Criteria: Assess if the candidate response mirrors the reference in terms of content, accuracy, and relevance.

  Score0: The candidate answer is refusing to give concrete results, providing something like “cannot decide”.
  Score0: The candidate answer is completely mis-aligned, providing incorrect or irrelevant information compared to the reference.
  Score1: The candidate answer shows minimal alignment, often misunderstanding or providing irrelevant details unrelated to the reference.
  Score2: The candidate answer recognizes the topic but diverges significantly from the reference in accuracy or relevance.
  Score3: The candidate answer aligns with the reference generally but lacks detail or precise accuracy in some aspects.
  Score4: The candidate answer is mostly accurate and relevant, closely following the reference but could be clearer or more detailed.
  Score5: The candidate answer is highly accurate, detailed, and matches the reference answer perfectly, capturing its essence and detail.

  Your response MUST be a single valid JSON object with the following fields and nothing else:
  {{
    "score": (int),
    "explanation": (string, provide a concise explanation of your rating, comparing the reference answer with the candidate's response. For example: "The reference answer is [XXX], while the candidate's answer is [YYY]. I think …")
  }}
  Do not return anything except this JSON object.

#for final score, 1 added so its on a scale of 1-10(then multiplied to scale to 100 like other judges)
callhome_judge_prompt: |-
  [Task]
  You are evaluating a model-generated 2-speaker candidate transcription against a 2-speaker reference transcription. Your job is to score the candidate on three dimensions:
  1. **ASR Efficacy** (0–3): How accurately the candidate reflects the spoken words of the reference, including correct words, minimal deletions/insertions, and overall transcription quality.
  2. **Semantic Similarity** (0–3): How well the candidate captures the *meaning* of the reference, even if exact words differ.
  3. **Turn-by-Turn Segmentation & Attribution Faithfulness** (0–3): How well the candidate preserves *who said what* and the structure of the speaker turns. This includes speaker consistency and proper segmentation of turns.

  Use the following scoring rubric for each category:

  Score 0: The candidate fails entirely in this dimension (e.g., refuses to answer, is completely wrong or irrelevant).  
  Score 1: Minimal alignment; severe issues or inaccuracies.  
  Score 2: Partial alignment; noticeable deviations from the reference but some correct content.  
  Score 3: Strong alignment; mostly accurate with only minor issues.

  Report the individual sub-scores for each dimension. Then, add the three sub-scores to compute the final **score** (range: 0 to 9). Be critical in your assessment. If the model says something like "I cannot decide", give a 0 for that dimension.

  Your response MUST be a **single valid JSON object** with the following fields and nothing else:

  {{
    "asr_score": (integer from 0 to 3),
    "semantic_score": (integer from 0 to 3),
    "turn_score": (integer from 0 to 3),
    "score": (integer from 0 to 9),
    "explanation": (string explaining your rating across all 3 criteria)
  }}
  Do not return anything except this JSON object.
