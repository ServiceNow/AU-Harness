binary_judge_prompt: |-
  [Task]
  Rate the model's answer based on its alignment with the reference answer, focusing on accuracy and relevance to the reference provided. Please be critical on the details.
  Criteria: Assess if the model's response mirrors the reference in terms of content, accuracy, and relevance. Please give a score of 0 or 1. 
  Score0: The answer is refusing to give concrete results, providing something like 'cannot decide'.
  Score0: The answer is wrong, providing incorrect or irrelevant information compared to the reference. 
  Score1: The answer is correct, capturing or covering the meaning from the reference.

  Your response MUST be a single valid JSON object with the following fields and nothing else:
  {{
    "score": (int),
    "explanation": (string, provide a concise explanation of your rating, comparing the reference answer with the model's response. For example: "The reference answer is [XXX], while the model's answer is [YYY]. I think …")
  }}
  Do not return anything except this JSON object.
detailed_judge_prompt: |-
  [Task]
  Rate the model's answer based on its alignment with the reference answer, focusing on accuracy and relevance to the reference provided. Please be critical on the details. If the model response is something like “cannot decide”, please rate as 0.
  Criteria: Assess if the model's response mirrors the reference in terms of content, accuracy, and relevance.

  Score0: The answer is refusing to give concrete results, providing something like “cannot decide”.
  Score0: The answer is completely mis-aligned, providing incorrect or irrelevant information compared to the reference.
  Score1: The answer shows minimal alignment, often misunderstanding or providing irrelevant details unrelated to the reference.
  Score2: The answer recognizes the topic but diverges significantly from the reference in accuracy or relevance.
  Score3: The answer aligns with the reference generally but lacks detail or precise accuracy in some aspects.
  Score4: The answer is mostly accurate and relevant, closely following the reference but could be clearer or more detailed.
  Score5: The answer is highly accurate, detailed, and matches the reference answer perfectly, capturing its essence and detail.

  Your response MUST be a single valid JSON object with the following fields and nothing else:
  {{
    "score": (int),
    "explanation": (string, provide a concise explanation of your rating, comparing the reference answer with the model's response. For example: "The reference answer is [XXX], while the model's answer is [YYY]. I think …")
  }}
  Do not return anything except this JSON object.

#for final score, 1 added so its on a scale of 1-10(then multiplied to scale to 100 like other judges)
callhome_judge_prompt: |-
  [Task]
  You are evaluating a model-generated candidate transcription against a 2-speaker reference transcription. Your job is to score the candidate on three dimensions:
  1. **ASR Efficacy** (0–3): How accurately the candidate reflects the spoken words of the reference, including correct words, minimal deletions/insertions, and overall transcription quality.
  2. **Semantic Similarity** (0–3): How well the candidate captures the *meaning* of the reference, even if exact words differ.
  3. **Turn-by-Turn Segmentation & Attribution Faithfulness** (0–3): How well the candidate preserves *who said what* and the structure of the speaker turns. This includes speaker consistency and proper segmentation of turns.

  Use the following scoring rubric for each category:

  Score 0: The candidate fails entirely in this dimension (e.g., refuses to answer, is completely wrong or irrelevant).  
  Score 1: Minimal alignment; severe issues or inaccuracies.  
  Score 2: Partial alignment; noticeable deviations from the reference but some correct content.  
  Score 3: Strong alignment; mostly accurate with only minor issues.

  Report the individual sub-scores for each dimension. Then, add the three sub-scores to compute the final **score** (range: 0 to 9). Be critical in your assessment. If the model says something like "I cannot decide", give a 0 for that dimension.

  Your response MUST be a **single valid JSON object** with the following fields and nothing else:

  {{
    "asr_score": (integer from 0 to 3),
    "semantic_score": (integer from 0 to 3),
    "turn_score": (integer from 0 to 3),
    "score": (integer from 0 to 9),
    "explanation": (string explaining your rating across all 3 criteria)
  }}
  Do not return anything except this JSON object.



big_bench_audio_judge_prompt: |-

    Assess whether the following CANDIDATE ANSWER is CORRECT or INCORRECT.
    For the CANDIDATE ANSWER to be correct, it must be consistent with the OFFICIAL ANSWER.
    If the CANDIDATE ANSWER contradicts itself, assess the first proposed answer.
    If the CANDIDATE ANSWER provides a final answer and working, assess the final answer only.
    If the CANDIDATE ANSWER includes irrelevant information, assess only the relevant information.
    If the CANDIDATE ANSWER includes a numeric value it is ok if it is spelled e.g. 7 or seven
    It is ok if the CANDIDATE ANSWER involves a misspelling of a person's name e.g. Leda or Lida, Autry or Audrie.

    The question, for reference only: START QUESTION {reference[0]} \n\nEND QUESTION

    The OFFICIAL ANSWER: {reference[1]}

    BEGIN CANDIDATE ANSWER TO ASSESS

    {prediction}

    END CANDIDATE ANSWER TO ASSESS

    Reply only with CORRECT or INCORRECT.