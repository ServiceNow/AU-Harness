
dataset_metric: #list of dataset-metric pairs, each as a two-item list of strings ["dataset_name", "metric_name"]
  # Speech recognition datasets
  - ["alpaca_audio_test", "llm_judge_detailed"]
  - ["librispeech_test_clean", "word_error_rate"]
  
  # Emotion recognition runspec
  - ["emotion_recognition", "llm_judge_binary"]
  
  # Gender recognition datasets
  - ["voxceleb_gender_test", "llm_judge_binary"]
  - ["iemocap_gender_recognition", "llm_judge_binary"]
  
  # Question-answering datasets and runspecs
  - ["big_bench_audio", "llm_judge_big_bench_audio"]
  - ["music_understanding", "llm_judge_binary"]

# Optional: Aggregate multiple dataset-metric pairs into a single score
# aggregate has x two-item lists, each as a two-item list of strings ["metric_name", ["dataset/runspec1", "dataset2", ..., "datasetN"]]
aggregate:
  - ["llm_judge_binary", ["emotion_recognition"]]
  - ["llm_judge_binary", ["voxceleb_gender_test", "iemocap_gender_recognition"]]
  
num_samples: 100 #number of samples to run(remove for all)
judge_concurrency: 8 #judge call(optional)
judge_model: "gpt-4o-mini" #optional
user_prompt_add_ons: ["asr_clean_output"] #optional - additional prompting in text instructions for each sample
length_filter: [1.0, 3.0] #optional - filters for only audio samples in this length(seconds) - only supported for general and callhome preprocessors
accented: false #optional - filters for only audio samples in this length(seconds)
language: "english" #optional - filters for only audio samples in this language

#override the requester temperature values by task or model
temperature_overrides:
  #model and task override
  - model: "gpt-4o-mini-audio-preview"
    task: "emotion_recognition"
    temperature: 0.9
  #model only override
  - model: "gpt-4o-mini-audio-preview"
    temperature: 0.3
  #task only override
  - task: "ASR"
    temperature: 0.5

models:
  - info:
      name: "gpt-4o-mini-audio-preview-1" 
      inference_type: "openai" #you can use vllm, openai, (chat completion) or audio transcription endpoint
      url: "${ENDPOINT_URL}" - #endpoint url
      delay: 100
      retry_attempts: 8
      timeout: 30
      model: "gpt-4o-mini-audio-preview" #only needed for vllm
      auth_token: "${AUTH_TOKEN}" 
      api_version: "${API_VERSION}"
      batch_size: 100 #batch eval size
      chunk_size: 45 #max audio length in seconds fed to model
  - info:
      name: "gpt-4o-mini-audio-preview-2" 
      inference_type: "openai" #you can use vllm, openai, (chat completion) or audio transcription endpoint
      url: "${ENDPOINT_URL}" - #endpoint url
      delay: 100
      retry_attempts: 8
      timeout: 30
      model: "gpt-4o-mini-audio-preview" #only needed for vllm
      auth_token: "${AUTH_TOKEN}" 
      api_version: "${API_VERSION}"
      batch_size: 100 #batch eval size
      chunk_size: 45 #max audio length in seconds fed to model
  - info:
      name: "qwen-2.5-omni" 
      inference_type: "vllm" #you can use vllm, openai, (chat completion) or audio transcription endpoint
      url: "${ENDPOINT_URL}" - #endpoint url
      delay: 100
      retry_attempts: 8
      timeout: 30
      model: "qwen-2.5-omni" #only needed for vllm
      auth_token: "${AUTH_TOKEN}" 
      batch_size: 4 #batch eval size
      chunk_size: 45 #max audio length in seconds fed to model

      #If two models have same "model" attribute, we implement dataset sharding

#In command line you can also pass custom config file name to read from with bash evaluate.sh --config <config_file>