
dataset_metric: # tuple of dataset, metric(can run by task or dataset)
  - "(wavcaps_qa_test, word_error_rate)"
  - "(alpaca_audio_test, word_error_rate)"
  - "(emotion_recognition, llm_judge_binary)"
num_samples: 100 # number of samples to run(remove for all)
user_prompt_add_ons: ["asr_clean_output"] # optional - additional prompting in text instructions for each sample
length_filter: [1.0, 3.0] # optional - filters for only audio samples in this length(seconds)
accented: false # optional - filters for only audio samples in this length(seconds)
language: "english" # optional - filters for only audio samples in this language

judge_concurrency: 250 # optional - default is 1
judge_model: "infer-qwen3-32b" # mandatory
judge_type: "vllm" # mandatory (vllm or openai)
#judge_api_version: "2025-01-01-preview" # optional(needed for openai)
judge_api_endpoint: "https://infer-qwen3-32b-runai-nowllm.inference.ta121237.dgxcloud.ai/v1" # mandatory
judge_api_key: "8o30OElfDYV_D6YbbznT0A:GDC2BsXIfSdfjv9iWka3V4MkazpvHfe0cCwXohzbP0Q" # mandatory
judge_temperature: 0.1 # optional
#judge_prompt_model_override: "Qwen3-32b" # optional

models:
  - info:
      name: "gpt-4o-mini-audio-preview-1" 
      inference_type: "openai" # mandatory - you can use vllm, openai, (chat completion) or audio transcription endpoint
      url: "${ENDPOINT_URL}" - # mandatory - endpoint url
      delay: 100
      retry_attempts: 8
      timeout: 30
      model: "gpt-4o-mini-audio-preview" # mandatory - only needed for vllm
      auth_token: "${AUTH_TOKEN}" 
      api_version: "${API_VERSION}"
      batch_size: 100 # Optional - batch eval size
      chunk_size: 45 # Optional - max audio length in seconds fed to model
  - info:
      name: "gpt-4o-mini-audio-preview-2" 
      inference_type: "openai" # mandatory - you can use vllm, openai, (chat completion) or audio transcription endpoint
      url: "${ENDPOINT_URL}" - # mandatory - endpoint url
      delay: 100
      retry_attempts: 8
      timeout: 30
      model: "gpt-4o-mini-audio-preview" # mandatory - only needed for vllm
      auth_token: "${AUTH_TOKEN}" 
      api_version: "${API_VERSION}"
      batch_size: 100 # Optional - batch eval size
      chunk_size: 45 # Optional - max audio length in seconds fed to model
  - info:
      name: "qwen-2.5-omni" 
      inference_type: "vllm" # mandatory - you can use vllm, openai, (chat completion) or audio transcription endpoint
      url: "${ENDPOINT_URL}" - # mandatory - endpoint url
      delay: 100
      retry_attempts: 8
      timeout: 30
      model: "qwen-2.5-omni" # mandatory - only needed for vllm
      auth_token: "${AUTH_TOKEN}" 
      batch_size: 4 # Optional - batch eval size
      chunk_size: 45 # Optional - max audio length in seconds fed to model

      # Data sharding - If two models have same "model" attribute, we implement dataset sharding

# In command line you can also pass custom config file name to read from with bash evaluate.sh --config <config_file>